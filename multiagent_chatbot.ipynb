{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tools.py\n",
    "\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "def search_process(id: str) -> str:\n",
    "    \"\"\"\n",
    "    Procura uma pasta de processo SEI no sistema de arquivos.\n",
    "\n",
    "    Use esta função para localizar documentos de processos administrativos pelo seu número de referência.\n",
    "    A função lida com formatos de ID tradicionais (XXX/YYYY) e compactos (XXXYYYY).\n",
    "\n",
    "    Args:\n",
    "        id (str): Número do processo em qualquer formato:\n",
    "            - Formato separado: \"XXX/YYYY\"\n",
    "            - Formato compacto: \"XXXYYYY\"\n",
    "            O número será automaticamente preenchido com zeros à esquerda se necessário.\n",
    "\n",
    "    Returns:\n",
    "        str: Um dos seguintes:\n",
    "            - Nome da pasta se o processo existir\n",
    "            - None se o processo não for encontrado\n",
    "            - \"Process not found\" se o processo não for encontrado (formato separado ou erros)\n",
    "    \"\"\"\n",
    "    root_path = os.path.abspath(\"\")\n",
    "    processes_path = os.path.join(root_path, \"processos\")\n",
    "    try:\n",
    "        if len(id) < 9:\n",
    "            if id.find(\"/\") == -1:\n",
    "                id = id.zfill(9)\n",
    "            else:\n",
    "                id = id.split(\"/\")\n",
    "                id[0] = id[0].zfill(5)\n",
    "                id[1] = id[1]\n",
    "                id = \"/\".join(id)\n",
    "        if id.find(\"/\") == -1:\n",
    "            folder = f\"SEI_{id[:-4]}_{id[-4:]}\"\n",
    "            # print(f\"Searching for {folder}\")\n",
    "            if os.path.exists(os.path.join(processes_path, folder)):\n",
    "                # print(f\"Process {id} found!\")\n",
    "                return folder\n",
    "            else:\n",
    "                # print(f\"Process {id} not found!\")\n",
    "                return \"Process not found\"\n",
    "        else:\n",
    "            folder = f\"SEI_{id.split('/')[0]}_{id.split('/')[1]}\"\n",
    "            # print(f\"Searching for {folder}\")\n",
    "            if os.path.exists(os.path.join(processes_path, folder)):\n",
    "                # print(f\"Process {id} found!\")\n",
    "                return folder\n",
    "            else:\n",
    "                # print(f\"Process {id} not found!\")\n",
    "                return \"Process not found\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Process not found\"\n",
    "\n",
    "def get_document_list_from_process(\n",
    "    parameters: str\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Recupera documentos PDF de uma pasta de processo SEI com suporte a paginação.\n",
    "\n",
    "    Use esta função para obter uma lista de documentos PDF dentro de uma pasta de processo.\n",
    "    Os resultados podem ser paginados usando parâmetros de limite e deslocamento.\n",
    "    Normalmente usado após localizar uma pasta de processo com search_process().\n",
    "\n",
    "    Args:\n",
    "        parameters (str): Uma string contendo o nome da pasta do processo e parâmetros de paginação.\n",
    "            A string deve ser formatada da seguinte forma:\n",
    "            \"process_folder,limit,offset\"\n",
    "            - process_folder: O nome da pasta do processo\n",
    "            - limit: O número máximo de documentos a retornar\n",
    "            - offset: O número de documentos a pular\n",
    "\n",
    "    Returns:\n",
    "        Union[dict(str : list[str], str : int), str]: Um dos seguintes:\n",
    "            - Um dicionário contendo:\n",
    "                - \"documents\": Uma lista de nomes de documentos PDF\n",
    "                - \"total_number_of_documents\": O número total de documentos na pasta\n",
    "            - \"Invalid parameters\" se a string de entrada não estiver formatada corretamente\n",
    "            - \"Process folder not found\" se a pasta do processo não existir\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process_folder, limit, offset = parameters.split(\",\")\n",
    "        limit = int(limit)\n",
    "        offset = int(offset)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Invalid parameters\"\n",
    "    try:\n",
    "        tree = os.walk(os.path.join(os.path.abspath(\"\"), \"processos\", process_folder))\n",
    "        documents = []\n",
    "        for root, dirs, files in tree:\n",
    "            documents.extend([\n",
    "                file\n",
    "                for file in files\n",
    "                if file.endswith(\".pdf\")\n",
    "                ])\n",
    "        documents.sort()\n",
    "        return {\n",
    "            \"documents\" : documents[offset:offset+limit],\n",
    "            \"total_number_of_documents\" : len(documents)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Process folder not found\"\n",
    "\n",
    "def read_doc(file_path: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Lê um documento PDF e extrai seu conteúdo de texto.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): O caminho para o arquivo PDF.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: O texto extraído do PDF, ou None se ocorrer um erro.\n",
    "    \"\"\"\n",
    "    from PyPDF2 import PdfReader\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        contents = []\n",
    "        for i in range(len(reader.pages)):\n",
    "            page = reader.pages[i]\n",
    "            contents.append(page.extract_text())\n",
    "        return \"\\n\".join(contents)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    \n",
    "def get_document_by_type(parameters : str) -> str:\n",
    "    \"\"\"\n",
    "    Obtem uma lista de documentos por tipo de um processo SEI.\n",
    "    \n",
    "    Args:\n",
    "        parameters (str): Uma string contendo o ID do processo e tipo de documento.\n",
    "            A string deve ser formatada da seguinte forma:\n",
    "            \"process_id,document_type\"\n",
    "            - process_id: O ID do processo (ex: \"XXX/YYYY\" ou \"XXXYYYY\") \n",
    "            - document_type: O tipo de documento a ser procurado\n",
    "    \"\"\"\n",
    "    process_id , document_type = parameters.split(\",\")\n",
    "    process_folder = search_process(process_id)\n",
    "    if process_folder == \"Process not found\" or process_folder == \"Process folder not found\":\n",
    "        return process_folder\n",
    "    response = get_document_list_from_process(f\"{process_folder},1,0\")\n",
    "    total_documents = response[\"total_number_of_documents\"]\n",
    "    response = get_document_list_from_process(f\"{process_folder},{total_documents},0\")\n",
    "    documents = response[\"documents\"]\n",
    "    documents_found = [\n",
    "        document\n",
    "        for document in documents\n",
    "        if document_type.lower() in document.lower()\n",
    "    ]\n",
    "    if len(documents_found) > 0:\n",
    "        return {\n",
    "            \"documents\" : documents_found,\n",
    "            \"number_of_documents\" : len(documents_found)\n",
    "        }\n",
    "    else:\n",
    "        return f\"Documents of type {document_type} not found in process {process_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MultiAgent.py\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from tools import *\n",
    "\n",
    "class MultiAgents:\n",
    "    def __init__(self, models):\n",
    "        self.supervisor_model, self.agent_model = self.initialize_models(models)\n",
    "        # self.chat_agent = self.initialize_agent(\n",
    "        #     name=\"chat_agent\",\n",
    "        #     tools=[],\n",
    "        #     prompt=\"You can only chat with the user.\"\n",
    "        # )\n",
    "        self.research_agent = self.initialize_agent(\n",
    "            name=\"sei_research_agent\",\n",
    "            tools=[search_process, get_document_list_from_process, get_document_by_type],\n",
    "            prompt=(\n",
    "                \"Você é especialista em obter informações sobre processos do SEI. \"\n",
    "                \"Você é capaz de: \"\n",
    "                \"pesquisar processos usando a função search_process, \"\n",
    "                \"listar documentos de um processo usando a função get_document_list_from_process, \"\n",
    "                \"e obter tipos específicos de documentos de um processo usando a função get_document_by_type. \"\n",
    "            )\n",
    "        )\n",
    "        self.graph = self.initialize_graph((\n",
    "            \"Você é um chatbot com um time de especialistas para atender o usuário. \"\n",
    "            \"Use sei_research_agent para responder perguntas sobre processos no \"\n",
    "            \"Sistema Eletrônico de Informações (SEI) do TRE do Rio Grande do Norte (TRE-RN).\"\n",
    "        ))\n",
    "\n",
    "    def initialize_models(self, models):\n",
    "        match models['supervisor'][\"provider\"]:\n",
    "            case \"groq\":\n",
    "                print(\"Using Groq model for supervisor\")\n",
    "                llm_supervisor = ChatGroq(\n",
    "                    model=models['supervisor']['model'],\n",
    "                    temperature=models['supervisor']['temperature']\n",
    "                )\n",
    "            case \"ollama\":\n",
    "                print(\"Using Ollama model for supervisor\")\n",
    "                llm_supervisor = ChatOllama(\n",
    "                    model=models['supervisor']['model'],\n",
    "                    temperature=models['supervisor']['temperature']\n",
    "                    )\n",
    "                model_name = models['supervisor']['model']\n",
    "                subprocess.run([\"ollama\", \"pull\", model_name])\n",
    "                \n",
    "        match models['agent']['provider']:\n",
    "            case \"groq\":\n",
    "                print(\"Using Groq model for agent\")\n",
    "                llm_agent = ChatGroq(\n",
    "                    model=models['agent']['model'],\n",
    "                    temperature=models['agent']['temperature']\n",
    "                )\n",
    "            case \"ollama\":\n",
    "                print(\"Using Ollama model for agent\")\n",
    "                llm_agent = ChatOllama(\n",
    "                    model=models['agent']['model'],\n",
    "                    temperature=models['agent']['temperature']\n",
    "                    )\n",
    "                model_name = models['agent']['model']\n",
    "                subprocess.run([\"ollama\", \"pull\", model_name])\n",
    "        return llm_supervisor, llm_agent\n",
    "    \n",
    "    def initialize_agent(self, name, prompt, tools):\n",
    "        agent =  create_react_agent(\n",
    "            self.agent_model,\n",
    "            name=name,\n",
    "            tools=tools,\n",
    "            prompt=prompt,\n",
    "        )\n",
    "        return agent\n",
    "    \n",
    "    def initialize_graph(self, prompt):\n",
    "        workflow = create_supervisor(\n",
    "            [self.research_agent],\n",
    "            model=self.supervisor_model,\n",
    "            prompt=prompt,\n",
    "            output_mode=\"full_history\"\n",
    "        )\n",
    "        return workflow.compile()\n",
    "    \n",
    "    def run(self, messages, recursion_limit=10):\n",
    "        return self.graph.invoke(\n",
    "            messages,\n",
    "            {\"recursion_limit\": recursion_limit}\n",
    "        )\n",
    "    \n",
    "    def stream(self, messages, recursion_limit=10):\n",
    "        return self.graph.stream(\n",
    "            messages,\n",
    "            {\"recursion_limit\": recursion_limit},\n",
    "            stream_mode=\"values\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Chatbot.py\n",
    "\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "import weave\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from MultiAgent import MultiAgents\n",
    "\n",
    "import logging, os\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\n",
    "    override=True\n",
    ")\n",
    "\n",
    "logging.info(\"Loaded environment variables\")\n",
    "logging.info(f\"GROQ_API_KEY: {os.getenv('GROQ_API_KEY')}\")\n",
    "logging.info(f\"WANDB_API_KEY: {os.getenv('WANDB_API_KEY')}\")\n",
    "\n",
    "# Initialize Weave\n",
    "weave.init(\"streamlit_sei\")\n",
    "\n",
    "def initialize_session_state():\n",
    "    \"\"\"Initialize the session state with a welcome message.\"\"\"\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state['messages'] = [\n",
    "            # {\n",
    "            #     \"role\": \"assistant\",\n",
    "            #     \"content\": \"Olá! Como posso ajudar você com o Sistema Eletrônico de Informações (SEI)?\"\n",
    "            # }\n",
    "        ]\n",
    "\n",
    "def get_message(message_data) -> Optional[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Process the message and return a tuple of (role, content).\n",
    "    \n",
    "    Args:\n",
    "        message_data: The message data from the stream\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing (role, content) or None if message should be skipped\n",
    "    \"\"\"\n",
    "    message = message_data[\"messages\"][-1]\n",
    "    \n",
    "    if isinstance(message, (HumanMessage, AIMessage, ToolMessage)):\n",
    "        role = \"user\" if isinstance(message, HumanMessage) else \"assistant\"\n",
    "        return role, message.content\n",
    "    return None\n",
    "\n",
    "def setup_agents():\n",
    "    \"\"\"Configure and return the MultiAgents setup.\"\"\"\n",
    "    models = {\n",
    "        \"supervisor\": {\n",
    "            \"provider\": \"groq\",\n",
    "            \"model\": \"llama-3.3-70b-versatile\",\n",
    "            \"temperature\": 0.0\n",
    "        },\n",
    "        \"agent\": {\n",
    "            \"provider\": \"groq\",\n",
    "            \"model\": \"llama3-8b-8192\",\n",
    "            \"temperature\": 0.0\n",
    "        },\n",
    "    }\n",
    "    return MultiAgents(models)\n",
    "\n",
    "def should_display_message(content: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the message should be displayed in the chat.\n",
    "    \"\"\"\n",
    "    skip_phrases = [\n",
    "        \"Successfully transferred\",\n",
    "        \"transferred to\",\n",
    "        \"transferred back\"\n",
    "    ]\n",
    "    return not any(phrase in content for phrase in skip_phrases)\n",
    "\n",
    "def main():\n",
    "    # Page configuration\n",
    "    st.title(\"💬 Chatbot SEI TRE-RN\")\n",
    "    st.caption(\"Um chatbot para responder perguntas sobre processos no Sistema Eletrônico de Informações (SEI) do Tribunal Regional Eleitoral do Rio Grande do Norte (TRE-RN).\")\n",
    "    \n",
    "    # Initialize session state\n",
    "    initialize_session_state()\n",
    "    \n",
    "    # Setup agents\n",
    "    agents = setup_agents()\n",
    "    \n",
    "    # Display chat history\n",
    "    for msg in st.session_state['messages']:\n",
    "        st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n",
    "    \n",
    "    # Handle user input\n",
    "    if prompt := st.chat_input():\n",
    "        # Add user message to chat\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        st.chat_message(\"user\").write(prompt)\n",
    "        \n",
    "        try:\n",
    "            # Stream the response\n",
    "            with st.spinner(\"Processando sua pergunta...\"):\n",
    "                stream = agents.stream({\"messages\": st.session_state['messages']})\n",
    "                \n",
    "                # Create a placeholder for the assistant's message\n",
    "                message_placeholder = st.chat_message(\"assistant\")\n",
    "                full_response = \"\"\n",
    "                assistant_content = \"\"\n",
    "                \n",
    "                # Add logging for debugging\n",
    "                logging.info(f\"Starting to process stream for prompt: {prompt[:30]}...\")\n",
    "                \n",
    "                for stream_data in stream:\n",
    "                    result = get_message(stream_data)\n",
    "                    if result is None:\n",
    "                        continue\n",
    "                    \n",
    "                    role, content = result\n",
    "                    logging.debug(f\"Received message - Role: {role}, Content length: {len(content)}\")\n",
    "                    \n",
    "                    if role == \"assistant\" and should_display_message(content):\n",
    "                        # For assistant messages, always use the latest complete chunk\n",
    "                        # This prevents partial messages from being displayed\n",
    "                        assistant_content = content\n",
    "                        # Update the displayed message with complete content\n",
    "                        message_placeholder.markdown(assistant_content)\n",
    "                \n",
    "                # Only append the final response to session state\n",
    "                if assistant_content:\n",
    "                    logging.info(f\"Final response length: {len(assistant_content)}\")\n",
    "                    st.session_state.messages.append({\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": assistant_content\n",
    "                    })\n",
    "                else:\n",
    "                    logging.warning(\"No assistant content was generated!\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during streaming: {str(e)}\")\n",
    "            st.error(f\"Ocorreu um erro: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q streamlit\n",
    "!npm install localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run Chatbot.py &>/content/logs.txt &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
